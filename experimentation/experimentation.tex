\chapter{Experimentación: modelos, \\entrenamiento y generación}

En este capítulo usaremos todos los conceptos vistos en los capítulos anteriores para justificar las elecciones de los diferentes modelos, hablaremos de las características principales de los mismos y finalmente entraremos en la fase del entrenamiento y los resultados de dichos modelos.





\input{preprocess/preprocess.tex}


\section{Resultados}

Habiendo preprocesado los datos, entrenado el modelo, guardado su estado y decidido el método de decodificación, estamos listos para poder generar comentarios. Se presentan aquí dos ejemplos generados al azar:

\begin{thm}
	\sffamily{
		Paediatric invasive immunosuppression (IPIS) in the treatment of recurrent kidney disease. Epidemiologic evidence suggests a association between immunosuppression and recurrence of recurrent kidney disease. Acute renal injury is a major risk factor for recurrent kidney disease. As the patient develops a family history of recurrence, his status will likely be monitored to determine whether he may benefit from continued immunosuppression.
	}
\end{thm}
\begin{thm}
	\sffamily{
		Predominant asymptomatic single cell carcinoma, melanoma, and cerebrovascular myopathy. The diagnosis of melanoma, cerebrovascular myopathy, and primary carcinoma requires extensive examination. Among these cancers, recurrent myopathy is a common occurrence. From 1980 to 1989, the incidence of melanoma, cerebrovascular myopathy, and secondary carcinoma increased from 8.5\% to 12.6\%.
	}
\end{thm}


Se puede apreciar que, a veces, los comentarios son muy cortos, como el comentario \ref{com:short}. En contadas ocasiones el generador ha devuelto una línea vacía. 

En ocasiones, los comentarios pueden no ser rigurosos médicamente hablando, pero esto, de cara al análisis de las herramientas disponibles para el procesamiento de los comentarios, no es estrictamente necesario. Necesitamos que los comentarios incluyan conceptos que sean importantes y destacables, como tipos de enfermedades, medicaciones, etc. De esta forma, podremos analizar cómo las herramientas detectan diferentes tipos de información y cómo de fiables son. 

Es por ello que poseer un generador de comentarios nos resulta conveniente, ya que se pueden considerar muchos más casos, prácticamente de forma ilimitada.

\subsection{Métricas y calidad de los comentarios}

Uno de los grandes retos de la creación de modelos de lenguaje, a parte de en sí ser el desarrollo de los mismos extremadamente complejo, es la creación de métricas de calidad de los resultados obtenidos.

Con modelos clásicos que trabajan con números, o incluso imágenes, existe una serie de herramientas que permiten ofrecer, de forma objetiva, rigurosa y consistente, un valor cuantitativo que mide la calidad de la salida de dicho modelo.

En los modelos de lenguaje, y más aún, en los modelos generativos, obtener un valor cuantitativo que evalúe la calidad de los comentarios generados es, más que complicado, subjetivo.

La pregunta que nos hacemos realmente es: ?`cómo de buenos son los comentarios generados? ?`Son comentarios coherentes? ?`Son comentarios rigurosos? ?`Cómo podemos evaluar de forma automática un conjunto de comentarios suficientemente representativo?

Para solucionar estos problemas, se han inventado algunas técnicas, anuque las más relevantes son la métrica BLEU \cite{BLEU} y la métrica Rouge \cite{lin2004rouge}.

\subsubsection{BLEU}
La métrica BLEU se ideó originalmente con objeto de medir traducciones automáticas. Dado un texto en un idioma y la traducción automática a otro, poder ofrecer un valor objetivo de cómo de bien se corresponden dichas oraciones. Esta tarea, como decíamos anteriormente, se compone de un importante carácter subjetivo, y es la razón por la que existen intérpretes y traductores que se encargan de este trabajo. La traducción de texto no es un mapeo directo de un conjunto de palabras de un idioma al de otro, ya que han de considerarse expresiones y valores culturales.

Dicho eso, disponer de, al menos, una medida que pueda servir de forma orientativa para evaluar la calidad de un traductor automático es, cuanto menos, conveniente.

El algoritmo en cuestión toma un conjunto de oraciones de referencia y otro conjunto de oraciones generadas que deseemos evaluar, y calcula un \textit{score} en función de cómo de probable es que una de las frases generadas pertenezca al conjunto de oraciones de referencia. Esta métrica también se utiliza extensivamente en evaluación de modelos generativos de lenguaje, como el nuestro, ya que la naturaleza de la evaluación se corresponde directamente con la nuestro caso de uso.


Para evaluar el modelo, tomamos 100 oraciones aleatorias de nuestro conjunto de datos, y generamos 100 oraciones con nuestro modelo. El valor obtenido es de 0.0036. 
